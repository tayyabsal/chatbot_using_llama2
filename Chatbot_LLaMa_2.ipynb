{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd3effb50ea2441db2c67b24bfa9def3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5066c244c6e449e09194a00bd389f32b",
              "IPY_MODEL_854490003e934def9d528f9ecfa4d4d6",
              "IPY_MODEL_552e1bba83de48d2b809911b7bf7a5a4"
            ],
            "layout": "IPY_MODEL_44896d7e23a64637af33ad8695049d94"
          }
        },
        "5066c244c6e449e09194a00bd389f32b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86f87fca57fa488883b861f987e496a7",
            "placeholder": "​",
            "style": "IPY_MODEL_fe90bcf350874650b8bf38f7fd2efd31",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "854490003e934def9d528f9ecfa4d4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2ccf3b10c1144dc8cb4116eab4ee86b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_975d1e931b43400daf69b3027b453dfe",
            "value": 2
          }
        },
        "552e1bba83de48d2b809911b7bf7a5a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df54989c80c7482c899c55950dff3447",
            "placeholder": "​",
            "style": "IPY_MODEL_db40123b6cb84aab91c31670d1f4b3b1",
            "value": " 2/2 [01:06&lt;00:00, 30.07s/it]"
          }
        },
        "44896d7e23a64637af33ad8695049d94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86f87fca57fa488883b861f987e496a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe90bcf350874650b8bf38f7fd2efd31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2ccf3b10c1144dc8cb4116eab4ee86b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "975d1e931b43400daf69b3027b453dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df54989c80c7482c899c55950dff3447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db40123b6cb84aab91c31670d1f4b3b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "In this Colab Notebook, we are going to explore Llama-2 7B, a model fine-tuned for generating text & chatting.\n",
        "\n",
        "By the end of this tutorial, you'll be able to interact with this model and use it to generate conversational responses.\n",
        "\n",
        "Whether you're curious about chatbot technology or simply want to see a machine-generated response to a particular question, this notebook will serve as a comprehensive guide.\n",
        "\n",
        "## Workflow\n",
        "1. **Installations**: We'll begin by setting up our environment with the required libraries.\n",
        "2. **Prerequisites**: Ensure we have access to the Llama-2 7B model on Hugging Face.\n",
        "3. **Loading the Model & Tokenizer**: Retrieve the model and tokenizer for our session.\n",
        "4. **Creating the Llama Pipeline**: Prepare our model for generating responses.\n",
        "5. **Interacting with Llama**: Prompt the model for answers and explore its capabilities.\n",
        "\n",
        "Let's dive in!\n",
        "\n",
        "**First, change runtime to GPU.**\n",
        "\n",
        "\n",
        "You can play with Llama-2 7B Chat here: https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat"
      ],
      "metadata": {
        "id": "bWOx7lgoYHZx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations\n",
        "\n",
        "Before we proceed, we need to ensure that the essential libraries are installed:\n",
        "- `Hugging Face Transformers`: Provides us with a straightforward way to use pre-trained models.\n",
        "- `PyTorch`: Serves as the backbone for deep learning operations.\n",
        "- `Accelerate`: Optimizes PyTorch operations, especially on GPU."
      ],
      "metadata": {
        "id": "DNrZtnUyYheg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch accelerate"
      ],
      "metadata": {
        "id": "aNTmMJIMYjiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111c427e-707b-4931-9f28-e25cccf20db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "\n",
        "To load our desired model, `meta-llama/Llama-2-7b-chat-hf`, we first need to authenticate ourselves on Hugging Face. This ensures we have the correct permissions to fetch the model.\n",
        "\n",
        "1. Gain access to the model on Hugging Face: [Link](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).\n",
        "2. Use the Hugging Face CLI to login and verify your authentication status.\n",
        "\n"
      ],
      "metadata": {
        "id": "BO2pb-EeZA95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "_6AfgF3_arYL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c344a408-5796-4872-d827-54bd41488f64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "id": "6lD_oW1uavGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7c36cf6-90f0-4ba7-de62-d53e84d3b01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "krisograbek\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Model & Tokenizer\n",
        "\n",
        "Here, we are preparing our session by loading both the Llama model and its associated tokenizer.\n",
        "\n",
        "The tokenizer will help in converting our text prompts into a format that the model can understand and process."
      ],
      "metadata": {
        "id": "xmJHSjx4abta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\" # meta-llama/Llama-2-7b-hf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)"
      ],
      "metadata": {
        "id": "jsBrtGpZYmcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57384457-9adc-466a-d07e-7b176ae3289c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:631: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Llama Pipeline\n",
        "\n",
        "We'll set up a pipeline for text generation.\n",
        "\n",
        "This pipeline simplifies the process of feeding prompts to our model and receiving generated text as output.\n",
        "\n",
        "*Note*: This cell takes 2-3 minutes to run"
      ],
      "metadata": {
        "id": "cKHfNL76a6qI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "llama_pipeline = pipeline(\n",
        "    \"text-generation\",  # LLM task\n",
        "    model=model,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "0AqJo1R_a9IM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "cd3effb50ea2441db2c67b24bfa9def3",
            "5066c244c6e449e09194a00bd389f32b",
            "854490003e934def9d528f9ecfa4d4d6",
            "552e1bba83de48d2b809911b7bf7a5a4",
            "44896d7e23a64637af33ad8695049d94",
            "86f87fca57fa488883b861f987e496a7",
            "fe90bcf350874650b8bf38f7fd2efd31",
            "b2ccf3b10c1144dc8cb4116eab4ee86b",
            "975d1e931b43400daf69b3027b453dfe",
            "df54989c80c7482c899c55950dff3447",
            "db40123b6cb84aab91c31670d1f4b3b1"
          ]
        },
        "outputId": "df75db2b-64fb-48d9-eecb-f485652bbb25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd3effb50ea2441db2c67b24bfa9def3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Responses\n",
        "\n",
        "With everything set up, let's see how Llama responds to some sample queries."
      ],
      "metadata": {
        "id": "tlbsuGikcsB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llama_response(prompt: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate a response from the Llama model.\n",
        "\n",
        "    Parameters:\n",
        "        prompt (str): The user's input/question for the model.\n",
        "\n",
        "    Returns:\n",
        "        None: Prints the model's response.\n",
        "    \"\"\"\n",
        "    sequences = llama_pipeline(\n",
        "        prompt,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        max_length=256,\n",
        "    )\n",
        "    print(\"Chatbot:\", sequences[0]['generated_text'])\n",
        "\n",
        "\n",
        "\n",
        "prompt = 'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "m8RwW7Axcu9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### More Queries"
      ],
      "metadata": {
        "id": "7HSx6ctIdbW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"I'm a programmer and Python is my favorite language because of it's simple syntax and variety of applications I can build with it.\\\n",
        "Based on that, what language should I learn next?\\\n",
        "Give me 5 recommendations\"\"\"\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "FoYglrLvmTN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to learn fast?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "hk3QokHDdgx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'I love basketball. Do you have any recommendations of team sports I might like?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "HNCpOybWdc65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'How to get rich?\\n'\n",
        "get_llama_response(prompt)"
      ],
      "metadata": {
        "id": "kzG2YoH4dil_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problems\n",
        "\n",
        "After 3-4 prompts, the model stops giving responses. It only outputs the user prompt.\n",
        "\n",
        "To keep talking to the model, you need to restart the notebook: `Runtime -> Restart Runtime` and run the notebook again..."
      ],
      "metadata": {
        "id": "xBfybeqxr3jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make it conversational\n",
        "Let's create an interactive chat loop, where you can converse with the Llama model.\n",
        "\n",
        "Type your questions or comments, and see how the model responds!"
      ],
      "metadata": {
        "id": "De1MFB-xgavO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"bye\", \"quit\", \"exit\"]:\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    get_llama_response(user_input)"
      ],
      "metadata": {
        "id": "a4xgy3NMgcwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c5OW9VH2lVb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion\n",
        "\n",
        "Thanks to the Hugging Face Library, creating a pipeline to chat with llama 2 (or any other open-source LLM) is quite easy.\n",
        "\n",
        "But if you worked a lot with much larger models such as GPT-4, you need to adjust your expectations."
      ],
      "metadata": {
        "id": "s3lxvhWKqfFd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWD3HFWlr2BE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}